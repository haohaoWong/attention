{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "# import keras_contrib.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\requests\\compat.py:11\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 11\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchardet\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'chardet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\__init__.py:51\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m autograph\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m bitwise\n\u001B[1;32m---> 51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compat\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:37\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"Compatibility functions.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \n\u001B[0;32m      5\u001B[0m \u001B[38;5;124;03mThe `tf.compat` module contains two sets of compatibility functions.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     32\u001B[0m \n\u001B[0;32m     33\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m v1\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m v2\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m forward_compatibility_horizon\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:31\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m autograph\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m bitwise\n\u001B[1;32m---> 31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compat\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:38\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m v1\n\u001B[1;32m---> 38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m v2\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m forward_compatibility_horizon\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m forward_compatible\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v2\\__init__.py:28\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# pylint: disable=g-bad-import-order\u001B[39;00m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compat\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __internal__\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __operators__\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m audio\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\__init__.py:33\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m autograph\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m bitwise\n\u001B[1;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compat\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\__init__.py:38\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m v1\n\u001B[1;32m---> 38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m v2\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m forward_compatibility_horizon\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m forward_compatible\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\v2\\__init__.py:37\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m debugging\n\u001B[1;32m---> 37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_api\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mv2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m errors\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\distribute\\__init__.py:182\u001B[0m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cluster_resolver\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m coordinator\n\u001B[1;32m--> 182\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcollective_all_reduce_strategy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CollectiveAllReduceStrategy \u001B[38;5;28;01mas\u001B[39;00m MultiWorkerMirroredStrategy\n\u001B[0;32m    184\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcross_device_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CrossDeviceOps\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\distribute\\experimental\\__init__.py:10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m coordinator\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m partitioners\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rpc\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcentral_storage_strategy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CentralStorageStrategy\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcollective_all_reduce_strategy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _CollectiveAllReduceStrategyExperimental \u001B[38;5;28;01mas\u001B[39;00m MultiWorkerMirroredStrategy\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\distribute\\experimental\\rpc\\__init__.py:8\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m\"\"\"Public API for tf.distribute.experimental.rpc namespace.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrpc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrpc_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Client\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrpc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrpc_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Server\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\experimental\\__init__.py:22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parameter_server_strategy\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tpu_strategy\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfailure_handling\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m failure_handling\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfailure_handling\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m preemption_watcher\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\failure_handling\\failure_handling.py:35\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute_lib\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m multi_worker_util\n\u001B[1;32m---> 35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfailure_handling\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m failure_handling_util\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m constant_op\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\failure_handling\\failure_handling_util.py:19\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmoves\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01murllib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m request\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\requests\\__init__.py:45\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01murllib3\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RequestsDependencyWarning\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__ \u001B[38;5;28;01mas\u001B[39;00m charset_normalizer_version\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\requests\\exceptions.py:9\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mrequests.exceptions\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m~~~~~~~~~~~~~~~~~~~\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \n\u001B[0;32m      5\u001B[0m \u001B[38;5;124;03mThis module contains the set of Requests' exceptions.\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01murllib3\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m BaseHTTPError\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m CompatJSONDecodeError\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mRequestException\u001B[39;00m(\u001B[38;5;167;01mIOError\u001B[39;00m):\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;124;03m\"\"\"There was an ambiguous exception that occurred while handling your\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;124;03m    request.\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\requests\\compat.py:13\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchardet\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[1;32m---> 13\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mchardet\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# -------\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Pythons\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# -------\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# Syntax sugar.\u001B[39;00m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\charset_normalizer\\__init__.py:23\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mCharset-Normalizer\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m~~~~~~~~~~~~~~\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;124;03m:license: MIT, see LICENSE for more details.\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_fp, from_path, from_bytes, normalize\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m detect\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__, VERSION\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\tf\\lib\\site-packages\\charset_normalizer\\api.py:10\u001B[0m\n\u001B[0;32m      7\u001B[0m     PathLike \u001B[38;5;241m=\u001B[39m Union[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mos.PathLike[str]\u001B[39m\u001B[38;5;124m'\u001B[39m]  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstant\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmd\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mess_ratio\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcharset_normalizer\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CharsetMatches, CharsetMatch\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m warn\n",
      "\u001B[1;31mAttributeError\u001B[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "train_data = 'data/ctb5.1-pos/ctb5.1-pos/train.tsv'\n",
    "test_data = 'data/ctb5.1-pos/ctb5.1-pos/test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install protobuf==3.20 -us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t', skip_blank_lines=False, header=None)\n",
    "    # 取出文本部分\n",
    "    content = data[0]\n",
    "    # 取出标签部分\n",
    "    label = data[1]\n",
    "    \n",
    "    return content, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "X_train, y_train = get_data(train_data)\n",
    "X_test, y_test = get_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理与格式转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 'AS', 'X', 'DEG', 'M', 'VP', 'NR', 'VV', 'VA', 'ETC', 'LC', 'NT', 'LB', 'DER', 'SB', 'VC', 'IJ', 'AD', 'VE', 'NN', 'DEV', 'CS', 'CD', 'MSP', 'JJ', 'P', 'BA', 'OD', 'FW', 'SP', 'DEC', 'NP', 'PN', 'DT', 'CC', 'PU']\n",
      "520125 36 {nan: 18426, 'AS': 4118, 'X': 6, 'DEG': 12337, 'M': 13790, 'VP': 1, 'NR': 30570, 'VV': 69858, 'VA': 7755, 'ETC': 1303, 'LC': 7782, 'NT': 9659, 'LB': 245, 'DER': 258, 'SB': 455, 'VC': 5404, 'IJ': 12, 'AD': 36430, 'VE': 3005, 'NN': 136643, 'DEV': 634, 'CS': 892, 'CD': 16182, 'MSP': 1336, 'JJ': 13234, 'P': 17606, 'BA': 755, 'OD': 1675, 'FW': 33, 'SP': 468, 'DEC': 12510, 'NP': 5, 'PN': 6644, 'DT': 5986, 'CC': 7355, 'PU': 76753}\n"
     ]
    }
   ],
   "source": [
    "# 构建标签字典\n",
    "labels = y_train.tolist() + y_test.tolist()\n",
    "labels_types = list(set(labels))\n",
    "print(labels_types)\n",
    "\n",
    "labels_dict = {}\n",
    "labels_index = {\"padded_label\" : 0}\n",
    "\n",
    "for index in range(len(labels_types)):\n",
    "    label = labels_types[index]\n",
    "    labels_dict.update({label: labels.count(label)})\n",
    "    labels_index.update({label: index+1})\n",
    "\n",
    "np.save('y_labels_index.npy', labels_index) \n",
    "print(len(labels), len(labels_dict), labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'padded_label': 0, nan: 1, 'AS': 2, 'X': 3, 'DEG': 4, 'M': 5, 'VP': 6, 'NR': 7, 'VV': 8, 'VA': 9, 'ETC': 10, 'LC': 11, 'NT': 12, 'LB': 13, 'DER': 14, 'SB': 15, 'VC': 16, 'IJ': 17, 'AD': 18, 'VE': 19, 'NN': 20, 'DEV': 21, 'CS': 22, 'CD': 23, 'MSP': 24, 'JJ': 25, 'P': 26, 'BA': 27, 'OD': 28, 'FW': 29, 'SP': 30, 'DEC': 31, 'NP': 32, 'PN': 33, 'DT': 34, 'CC': 35, 'PU': 36}\n"
     ]
    }
   ],
   "source": [
    "# 构建的Y标签字典\n",
    "print(labels_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以句子进行拆分后的句子为：\n",
      " [array(['上海', '浦东', '开发', '与', '法制', '建设', '同步'], dtype=object), array(['新华社', '上海', '二月', '十日', '电', '（', '记者', '谢金虎', '、', '张持坚', '）'],\n",
      "      dtype=object), array(['上海', '浦东', '近年', '来', '颁布', '实行', '了', '涉及', '经济', '、', '贸易', '、',\n",
      "       '建设', '、', '规划', '、', '科技', '、', '文教', '等', '领域', '的', '七十一', '件',\n",
      "       '法规性', '文件', '，', '确保', '了', '浦东', '开发', '的', '有序', '进行', '。'],\n",
      "      dtype=object), array(['浦东', '开发', '开放', '是', '一', '项', '振兴', '上海', '，', '建设', '现代化',\n",
      "       '经济', '、', '贸易', '、', '金融', '中心', '的', '跨世纪', '工程', '，', '因此',\n",
      "       '大量', '出现', '的', '是', '以前', '不', '曾', '遇到', '过', '的', '新', '情况',\n",
      "       '、', '新', '问题', '。'], dtype=object), array(['对', '此', '，', '浦东', '不', '是', '简单', '的', '采取', '“', '干', '一', '段',\n",
      "       '时间', '，', '等', '积累', '了', '经验', '以后', '再', '制定', '法规', '条例', '”',\n",
      "       '的', '做法', '，', '而', '是', '借鉴', '发达', '国家', '和', '深圳', '等', '特区',\n",
      "       '的', '经验', '教训', '，', '聘请', '国内外', '有关', '专家', '学者', '，', '积极',\n",
      "       '、', '及时', '地', '制定', '和', '推出', '法规性', '文件', '，', '使', '这些', '经济',\n",
      "       '活动', '一', '出现', '就', '被', '纳入', '法制', '轨道', '。'], dtype=object)]\n",
      "以句子进行拆分后的句子所对应的词性为：\n",
      " [array(['NR', 'NR', 'NN', 'CC', 'NN', 'NN', 'VV'], dtype=object), array(['NN', 'NR', 'NT', 'NT', 'NN', 'PU', 'NN', 'NR', 'PU', 'NR', 'PU'],\n",
      "      dtype=object), array(['NR', 'NR', 'NT', 'LC', 'VV', 'VV', 'AS', 'VV', 'NN', 'PU', 'NN',\n",
      "       'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'PU', 'NN', 'ETC', 'NN', 'DEC',\n",
      "       'CD', 'M', 'NN', 'NN', 'PU', 'VV', 'AS', 'NR', 'NN', 'DEG', 'JJ',\n",
      "       'NN', 'PU'], dtype=object), array(['NR', 'NN', 'NN', 'VC', 'CD', 'M', 'VV', 'NR', 'PU', 'VV', 'NN',\n",
      "       'NN', 'PU', 'NN', 'PU', 'NN', 'NN', 'DEC', 'JJ', 'NN', 'PU', 'AD',\n",
      "       'AD', 'VV', 'DEC', 'VC', 'NT', 'AD', 'AD', 'VV', 'AS', 'DEC', 'JJ',\n",
      "       'NN', 'PU', 'JJ', 'NN', 'PU'], dtype=object), array(['P', 'PN', 'PU', 'NR', 'AD', 'VC', 'VA', 'DEV', 'VV', 'PU', 'VV',\n",
      "       'CD', 'M', 'NN', 'PU', 'P', 'VV', 'AS', 'NN', 'LC', 'AD', 'VV',\n",
      "       'NN', 'NN', 'PU', 'DEC', 'NN', 'PU', 'CC', 'VC', 'VV', 'JJ', 'NN',\n",
      "       'CC', 'NR', 'ETC', 'NN', 'DEG', 'NN', 'NN', 'PU', 'VV', 'NN', 'JJ',\n",
      "       'NN', 'NN', 'PU', 'AD', 'PU', 'AD', 'DEV', 'VV', 'CC', 'VV', 'NN',\n",
      "       'NN', 'PU', 'VV', 'DT', 'NN', 'NN', 'AD', 'VV', 'AD', 'SB', 'VV',\n",
      "       'NN', 'NN', 'PU'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# 按句对X、y进行拆分\n",
    "def split_corpus_by_sentence(content):\n",
    "    cleaned_sentence = []\n",
    "    split_label = content.isnull()\n",
    "    last_split_index = 0\n",
    "    index = 0\n",
    "    while index < len(content):\n",
    "        current_word = content[index]\n",
    "        if split_label[index] == True and len(cleaned_sentence) == 0:\n",
    "            cleaned_sentence.append(np.array(content[last_split_index:index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "        elif split_label[index] == True  and len(cleaned_sentence) > 0:\n",
    "            cleaned_sentence.append(np.array(content[last_split_index:index]))\n",
    "            last_split_index = index + 1\n",
    "            index += 1\n",
    "        else:\n",
    "            index += 1\n",
    "    return cleaned_sentence\n",
    "\n",
    "X_train_sent_split = split_corpus_by_sentence(X_train)\n",
    "y_train_sent_split = split_corpus_by_sentence(y_train)\n",
    "X_test_sent_split = split_corpus_by_sentence(X_test)\n",
    "y_test_sent_split = split_corpus_by_sentence(y_test)\n",
    "\n",
    "print('以句子进行拆分后的句子为：\\n', X_train_sent_split[:5])\n",
    "print('以句子进行拆分后的句子所对应的词性为：\\n', y_train_sent_split[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 6, 19, 34, 19, 19, 7], [19, 6, 11, 11, 19, 35, 19, 6, 35, 6, 35], [6, 6, 11, 10, 7, 7, 1, 7, 19, 35, 19, 35, 19, 35, 19, 35, 19, 35, 19, 9, 19, 30, 22, 4, 19, 19, 35, 7, 1, 6, 19, 3, 24, 19, 35], [6, 19, 19, 15, 22, 4, 7, 6, 35, 7, 19, 19, 35, 19, 35, 19, 19, 30, 24, 19, 35, 17, 17, 7, 30, 15, 11, 17, 17, 7, 1, 30, 24, 19, 35, 24, 19, 35], [25, 32, 35, 6, 17, 15, 8, 20, 7, 35, 7, 22, 4, 19, 35, 25, 7, 1, 19, 10, 17, 7, 19, 19, 35, 30, 19, 35, 34, 15, 7, 24, 19, 34, 6, 9, 19, 3, 19, 19, 35, 7, 19, 24, 19, 19, 35, 17, 35, 17, 20, 7, 34, 7, 19, 19, 35, 7, 33, 19, 19, 17, 7, 17, 14, 7, 19, 19, 35]]\n"
     ]
    }
   ],
   "source": [
    "def transfer_label_category_index(origin_labels, labels_types):\n",
    "    transfered_label = []\n",
    "    for sentence_labels in origin_labels:\n",
    "        labels_format_index = [labels_types.index(label) for label in sentence_labels]  # 将标签依据字典转化为序号\n",
    "        transfered_label.append(labels_format_index)\n",
    "    return transfered_label\n",
    "\n",
    "y_train_index = transfer_label_category_index(y_train_sent_split, labels_types)\n",
    "y_test_index = transfer_label_category_index(y_test_sent_split, labels_types)\n",
    "\n",
    "print(y_train_index[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# 标签格式转化\n",
    "# 构建对应（标签样本数，句子长度，标签类别数+1）形状的张量，值全为0\n",
    "y_train_index_padded = np.zeros((len(y_train_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), dtype='float', order='C')\n",
    "y_test_index_padded = np.zeros((len(y_test_index), MAX_SEQUENCE_LENGTH, len(labels_types)+1), dtype='float', order='C')\n",
    "\n",
    "# 填充张量\n",
    "for sentence_labels_index in range(len(y_train_index)):\n",
    "    for label_index in range(len(y_train_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_train_index_padded[sentence_labels_index, label_index, y_train_index[sentence_labels_index][label_index]+1] = 1\n",
    "    \n",
    "    if len(y_train_index[sentence_labels_index]) < MAX_SEQUENCE_LENGTH:\n",
    "        for label_index in range(len(y_train_index[sentence_labels_index]), MAX_SEQUENCE_LENGTH):\n",
    "            y_train_index_padded[sentence_labels_index, label_index, 0] = 1\n",
    "\n",
    "# 优化：若为填充的标签，则将其预测为第一位为1\n",
    "\n",
    "for sentence_labels_index in range(len(y_test_index)):\n",
    "    for label_index in range(len(y_test_index[sentence_labels_index])):\n",
    "        if label_index < MAX_SEQUENCE_LENGTH:\n",
    "            y_test_index_padded[sentence_labels_index, label_index, y_test_index[sentence_labels_index][label_index]+1] = 1\n",
    "    \n",
    "    if len(y_test_index[sentence_labels_index]) < MAX_SEQUENCE_LENGTH:\n",
    "        for label_index in range(len(y_test_index[sentence_labels_index]), MAX_SEQUENCE_LENGTH):\n",
    "            y_test_index_padded[sentence_labels_index, label_index, 0] = 1\n",
    "\n",
    "print(y_train_index_padded[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec模型导入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练的word2vec模型采用前人用中文维基百科训练好的模型，请各位同学进入链接下载，并放到相对本脚本 同一级文件夹data 的目录下 并解压。\n",
    "\n",
    "[word2vec模型链接](https://github.com/Embedding/Chinese-Word-Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 导入 预训练的词向量\n",
    "myPath = './data/sgns.wiki.word' # 本地词向量的地址\n",
    "Word2VecModel = gensim.models.KeyedVectors.load_word2vec_format(myPath) # 读取词向量，以二进制读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10168\\2531751338.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  vocab_list = [word for word, Vocab in Word2VecModel.wv.vocab.items()]# 存储 所有的 词语\n"
     ]
    }
   ],
   "source": [
    "## 2 构造包含所有词语的 list，以及初始化 “词语-序号”字典 和 “词向量”矩阵\n",
    "vocab_list = [word for word, Vocab in Word2VecModel.wv.vocab.items()]# 存储 所有的 词语\n",
    "\n",
    "word_index = {\" \": 0}# 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。\n",
    "word_vector = {} # 初始化`[word : vector]`字典\n",
    "\n",
    "# 初始化存储所有向量的大矩阵，留意其中多一位（首行），词向量全为 0，用于 padding补零。\n",
    "# 行数 为 所有单词数+1 比如 10000+1 ； 列数为 词向量“维度”比如100。\n",
    "embeddings_matrix = np.zeros((len(vocab_list) + 1, Word2VecModel.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10168\\1270982749.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  word_vector[word] = Word2VecModel.wv[word] # 词语：词向量\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_10168\\1270982749.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  embeddings_matrix[i + 1] = Word2VecModel.wv[word]  # 词向量矩阵\n"
     ]
    }
   ],
   "source": [
    "## 3 填充 上述 的字典 和 大矩阵\n",
    "for i in range(len(vocab_list)):\n",
    "    # print(i)\n",
    "    word = vocab_list[i]  # 每个词语\n",
    "    word_index[word] = i + 1 # 词语：序号\n",
    "    word_vector[word] = Word2VecModel.wv[word] # 词语：词向量\n",
    "    embeddings_matrix[i + 1] = Word2VecModel.wv[word]  # 词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2 构造包含所有词语的 list，以及初始化 “词语-序号”字典 和 “词向量”矩阵\n",
    "# word_index = {\" \": 0}# 初始化 `[word : token]` ，后期 tokenize 语料库就是用该词典。\n",
    "# word_vector = {} # 初始化`[word : vector]`字典\n",
    "\n",
    "# # 初始化存储所有向量的大矩阵，留意其中多一位（首行），词向量全为 0，用于 padding补零。\n",
    "# # 行数 为 所有单词数+1 比如 10000+1 ； 列数为 词向量“维度”比如100。\n",
    "# embeddings_matrix = np.zeros((len(Word2VecModel.index_to_key) + 1, Word2VecModel.vector_size))\n",
    "\n",
    "# ## 3 填充 上述 的字典 和 大矩阵\n",
    "# for i in range(len(Word2VecModel.index_to_key)):\n",
    "#     # print(i)\n",
    "#     word = Word2VecModel.index_to_key[i]  # 每个词语\n",
    "#     word_index[word] = i + 1 # 词语：序号\n",
    "#     word_vector[word] = Word2VecModel[word] # 词语：词向量\n",
    "#     embeddings_matrix[i + 1] = Word2VecModel[word]  # 词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_word_index.npy', word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  347 16980   507    10 15537   603  4380     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "# 序号化 文本，tokenizer句子，并返回每个句子所对应的词语索引\n",
    "\n",
    "# 由于将词语转化为索引的word_index需要与词向量模型对齐，故在导入词向量模型后再将X进行处理\n",
    "def tokenizer(texts, word_index):\n",
    "    data = []\n",
    "    for sentence in texts:\n",
    "        new_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                new_sentence.append(word_index[word])  # 把文本中的 词语转化为index\n",
    "            except:\n",
    "                new_sentence.append(0)\n",
    "       \n",
    "        data.append(new_sentence)\n",
    "    # 使用kears的内置函数padding对齐句子,好处是输出numpy数组，不用自己转化了\n",
    "    data = sequence.pad_sequences(data, maxlen = MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    \n",
    "    return data\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train_sent_split, word_index)\n",
    "X_test_tokenized = tokenizer(X_test_sent_split, word_index)\n",
    "\n",
    "print(X_train_tokenized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标引网络构建及训练评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\ml\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import keras\n",
    "from keras import optimizers\n",
    "\n",
    "EMBEDDING_DIM = 300 #词向量维度\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                    output_dim = EMBEDDING_DIM, # 词向量 长度（300）\n",
    "                    weights=[embeddings_matrix], # 重点：预训练的词向量系数\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, # 每句话的 最大长度（必须padding） \n",
    "                    trainable=False # 是否在 训练的过程中 更新词向量\n",
    "                   ))\n",
    "# input shape (Batch_size, Time_step, Input_Sizes)\n",
    "model.add(LSTM(128, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, input_shape=(MAX_SEQUENCE_LENGTH, 128), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(labels_types)+1, input_shape=(MAX_SEQUENCE_LENGTH, 64), activation='softmax'))\n",
    "\n",
    "adam = optimizers.adam_v2.Adam(lr=0.01, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          105665400 \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100, 128)          219648    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100, 64)           8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100, 37)           2405      \n",
      "=================================================================\n",
      "Total params: 105,895,709\n",
      "Trainable params: 230,309\n",
      "Non-trainable params: 105,665,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18078, 100) (18078, 100, 37)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tokenized.shape, y_train_index_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "142/142 [==============================] - 34s 219ms/step - loss: 0.3933 - accuracy: 0.8906\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 31s 221ms/step - loss: 0.1581 - accuracy: 0.9504\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 32s 228ms/step - loss: 0.1356 - accuracy: 0.9578\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 31s 215ms/step - loss: 0.1243 - accuracy: 0.9613\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 31s 216ms/step - loss: 0.1170 - accuracy: 0.9634\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 31s 216ms/step - loss: 0.1118 - accuracy: 0.9649\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 31s 218ms/step - loss: 0.1081 - accuracy: 0.9660\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 31s 217ms/step - loss: 0.1043 - accuracy: 0.9669\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 31s 221ms/step - loss: 0.1016 - accuracy: 0.9680\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 31s 218ms/step - loss: 0.0995 - accuracy: 0.9686\n",
      "evaluation!\n",
      "3/3 [==============================] - 1s 85ms/step - loss: 0.0555 - accuracy: 0.9820\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_tokenized, y_train_index_padded,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          verbose=1\n",
    "         )\n",
    "print('evaluation!')\n",
    "score = model.evaluate(X_test_tokenized, y_test_index_padded, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cn_pos_tag.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练的模型经在测试集上验证获得的loss和accuracy为：\n",
      "[0.05551695078611374, 0.9819827675819397]\n"
     ]
    }
   ],
   "source": [
    "print('训练的模型经在测试集上验证获得的loss和accuracy为：')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.06635525e-05 3.57727771e-07 1.05397652e-04 ... 2.33568018e-03\n",
      "   6.92393598e-07 3.12632910e-05]\n",
      "  [6.14345424e-07 1.82051492e-11 1.25356635e-07 ... 8.37220568e-07\n",
      "   1.73747974e-10 4.92240506e-05]\n",
      "  [1.05229647e-13 1.94991093e-15 7.08194614e-10 ... 8.24436630e-10\n",
      "   6.79707114e-07 2.06903022e-10]\n",
      "  ...\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 9.38489382e-15]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 9.38485824e-15]\n",
      "  [1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "   0.00000000e+00 9.38482266e-15]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test_tokenized[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_index_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.2263517e-15 1.9288053e-24 1.4051022e-15 ... 2.3026050e-22\n",
      "   9.8017287e-13 9.9999928e-01]\n",
      "  [2.4927356e-06 1.1019267e-08 1.0667166e-05 ... 7.5662494e-05\n",
      "   3.4589000e-08 3.9010945e-05]\n",
      "  [3.7808437e-08 7.0922929e-14 3.6413925e-09 ... 2.6348717e-08\n",
      "   1.0428908e-12 5.3828903e-06]\n",
      "  ...\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 9.3853953e-15]\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 9.3853233e-15]\n",
      "  [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "   0.0000000e+00 9.3852165e-15]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(X_test_tokenized[2:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_index_padded[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "142/142 [==============================] - 17s 112ms/step - loss: 0.7128 - accuracy: 0.8094\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 15s 105ms/step - loss: 0.3378 - accuracy: 0.8940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b5c03931f0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import GRU, SimpleRNN\n",
    "\n",
    "model_RNN = Sequential()\n",
    "model_RNN.add(Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                    output_dim = EMBEDDING_DIM, # 词向量 长度（300）\n",
    "                    weights=[embeddings_matrix], # 重点：预训练的词向量系数\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, # 每句话的 最大长度（必须padding） \n",
    "                    trainable=False # 是否在 训练的过程中 更新词向量\n",
    "                   ))\n",
    "# input shape (Batch_size, Time_step, Input_Sizes)\n",
    "model_RNN.add(SimpleRNN(128, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), activation='tanh', return_sequences=True))\n",
    "model_RNN.add(Dropout(0.5))\n",
    "model_RNN.add(Dense(64, input_shape=(MAX_SEQUENCE_LENGTH, 128), activation='relu'))\n",
    "model_RNN.add(Dropout(0.5))\n",
    "model_RNN.add(Dense(len(labels_types)+1, input_shape=(MAX_SEQUENCE_LENGTH, 64), activation='softmax'))\n",
    "\n",
    "adam = optimizers.adam_v2.Adam(lr=0.01, decay=1e-6)\n",
    "model_RNN.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_RNN.fit(X_train_tokenized, y_train_index_padded,\n",
    "          epochs=2,\n",
    "          batch_size=128,\n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "142/142 [==============================] - 30s 198ms/step - loss: 0.3547 - accuracy: 0.9104\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 27s 191ms/step - loss: 0.1603 - accuracy: 0.9481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b5c0b79a60>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GRU = Sequential()\n",
    "model_GRU.add(Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                    output_dim = EMBEDDING_DIM, # 词向量 长度（300）\n",
    "                    weights=[embeddings_matrix], # 重点：预训练的词向量系数\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, # 每句话的 最大长度（必须padding） \n",
    "                    trainable=False # 是否在 训练的过程中 更新词向量\n",
    "                   ))\n",
    "# input shape (Batch_size, Time_step, Input_Sizes)\n",
    "model_GRU.add(GRU(128, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), activation='tanh', return_sequences=True))\n",
    "model_GRU.add(Dropout(0.5))\n",
    "model_GRU.add(Dense(64, input_shape=(MAX_SEQUENCE_LENGTH, 128), activation='relu'))\n",
    "model_GRU.add(Dropout(0.5))\n",
    "model_GRU.add(Dense(len(labels_types)+1, input_shape=(MAX_SEQUENCE_LENGTH, 64), activation='softmax'))\n",
    "\n",
    "adam = optimizers.adam_v2.Adam(lr=0.01, decay=1e-6)\n",
    "model_GRU.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_GRU.fit(X_train_tokenized, y_train_index_padded,\n",
    "          epochs=2,\n",
    "          batch_size=128,\n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 300)     105665400   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bilstm (Bidirectional)          (None, 100, 200)     320800      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 200, 100)     0           bilstm[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 200, 100)     10100       permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 100, 200)     0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Multiply)        (None, 100, 200)     0           bilstm[0][0]                     \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 100, 200)     0           attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 100, 37)      7437        dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 106,003,737\n",
      "Trainable params: 338,337\n",
      "Non-trainable params: 105,665,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Training————\n",
      "142/142 [==============================] - 62s 406ms/step - loss: 3.1649 - accuracy: 0.7544\n",
      "Testing————–\n",
      "11/11 [==============================] - 1s 45ms/step - loss: 2.7294 - accuracy: 0.7789\n",
      "test loss: 2.729379415512085\n",
      "test accuracy: 0.7788505554199219\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "#from keras.optimizers import Adam\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "lstm_units = 100\n",
    "\n",
    "# first way attention\n",
    "def attention_3d_block(inputs):\n",
    "    #input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(MAX_SEQUENCE_LENGTH, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    #output_attention_mul = merge([inputs, a_probs], name=’attention_mul’, mode=’mul’)\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "# build RNN model with attention\n",
    "# inputs = Input(shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM))\n",
    "inputs = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n",
    "embeding1=Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                    output_dim = EMBEDDING_DIM, # 词向量 长度（300）\n",
    "                    weights=[embeddings_matrix], # 重点：预训练的词向量系数\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, # 每句话的 最大长度（必须padding）\n",
    "                    trainable=False # 是否在 训练的过程中 更新词向量\n",
    "                                  )(inputs)\n",
    "drop1 = Dropout(0.3)(embeding1)\n",
    "lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True), name='bilstm')(embeding1)\n",
    "attention_mul = attention_3d_block(lstm_out)\n",
    "# attention_flatten = Flatten()(attention_mul)\n",
    "drop2 = Dropout(0.3)(attention_mul)\n",
    "output = Dense(len(labels_types)+1, activation='sigmoid')(drop2)\n",
    "model_att = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model_att.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model_att.summary())\n",
    "\n",
    "print('Training————')\n",
    "att = model_att.fit(X_train_tokenized, y_train_index_padded, epochs=1, batch_size=128)\n",
    "\n",
    "print('Testing————–')\n",
    "loss, accuracy = model_att.evaluate(X_test_tokenized, y_test_index_padded)\n",
    "\n",
    "print('test loss:', loss)\n",
    "print('test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 100, 300)     105665400   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 100, 128)     219648      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 100, 128)     16512       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 100, 128)     16512       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 100, 128)     0           dense_10[0][0]                   \n",
      "                                                                 dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 100, 64)      8256        multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 100, 64)      0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 100, 37)      2405        dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 105,928,733\n",
      "Trainable params: 263,333\n",
      "Non-trainable params: 105,665,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "142/142 [==============================] - 39s 252ms/step - loss: 0.7412 - accuracy: 0.8115\n",
      "Epoch 2/2\n",
      "142/142 [==============================] - 36s 256ms/step - loss: 0.3091 - accuracy: 0.9009\n",
      "evaluation!\n",
      "3/3 [==============================] - 1s 101ms/step - loss: 0.1286 - accuracy: 0.9539\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1863 - accuracy: 0.9374\n",
      "训练的模型经在测试集上验证获得的loss和accuracy为：\n",
      "[0.12861023843288422, 0.953936755657196]\n",
      "训练的模型经在训练集集上验证获得的loss和accuracy为：\n",
      "[0.18626704812049866, 0.9373680949211121]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout,SimpleRNN,GRU,Bidirectional,Input,Flatten,Multiply\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 300 #词向量维度\n",
    "\n",
    "#两个维度变换过了\n",
    "input1=Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "embeding1=Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "                    output_dim = EMBEDDING_DIM, # 词向量 长度（300）\n",
    "                    weights=[embeddings_matrix], # 重点：预训练的词向量系数\n",
    "                    input_length=MAX_SEQUENCE_LENGTH, # 每句话的 最大长度（必须padding）\n",
    "                    trainable=False # 是否在 训练的过程中 更新词向量\n",
    "                                  )(input1)\n",
    "LSTM1=LSTM(128, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), return_sequences=True)(embeding1)\n",
    "dense1=Dense(128, input_shape=(MAX_SEQUENCE_LENGTH,128), activation='softmax')(LSTM1)\n",
    "dense2=Dense(128, input_shape=(MAX_SEQUENCE_LENGTH,128), activation='relu')(LSTM1)\n",
    "attention=Multiply()([dense1,dense2])\n",
    "dense3=Dense(64, activation='relu')(attention)\n",
    "dropout2=Dropout(0.5)(dense3)\n",
    "out=Dense(len(labels_types)+1, activation='softmax')(dropout2)\n",
    "model2=keras.models.Model(inputs=input1,outputs=out)\n",
    "\n",
    "adam=keras.optimizers.adam_v2.Adam(lr=0.01,decay=0.0001)\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model2.summary())\n",
    "\n",
    "model2.fit(X_train_tokenized, y_train_index_padded,\n",
    "          epochs=2,\n",
    "          batch_size=128,\n",
    "          verbose=1\n",
    "         )\n",
    "print('evaluation!')\n",
    "score1 = model2.evaluate(X_test_tokenized, y_test_index_padded, batch_size=128)\n",
    "score2 = model2.evaluate(X_train_tokenized, y_train_index_padded, batch_size=128)\n",
    "print('训练的模型经在测试集上验证获得的loss和accuracy为：')\n",
    "print(score1)\n",
    "print('训练的模型经在训练集集上验证获得的loss和accuracy为：')\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
